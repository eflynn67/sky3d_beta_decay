<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.20"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Sky3D: parallel Module Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Sky3D
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.20 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('namespaceparallel.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions/Subroutines</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">parallel Module Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This module organizes the execution on distributed-memory machines using <code>MPI</code>. Its interface is made such that on sequential machines <code><a class="el" href="parallel_8f90.html">parallel.f90</a></code> can simply be replaced by a special version <code><a class="el" href="sequential_8f90.html">sequential.f90</a></code> which contains a definition of this module generating trivial data.  
<a href="namespaceparallel.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions/Subroutines</h2></td></tr>
<tr class="memitem:af4d1baf7c89b4eb0a8ea52b0d0dffa04"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#af4d1baf7c89b4eb0a8ea52b0d0dffa04">alloc_nodes</a></td></tr>
<tr class="memdesc:af4d1baf7c89b4eb0a8ea52b0d0dffa04"><td class="mdescLeft">&#160;</td><td class="mdescRight">This subroutine merely allocates the internal arrays of module <code>Parallel</code>.  <a href="namespaceparallel.html#af4d1baf7c89b4eb0a8ea52b0d0dffa04">More...</a><br /></td></tr>
<tr class="separator:af4d1baf7c89b4eb0a8ea52b0d0dffa04"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa3a47cbcb6f4f85c1cf754b1d85a088b"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#aa3a47cbcb6f4f85c1cf754b1d85a088b">init_all_mpi</a></td></tr>
<tr class="memdesc:aa3a47cbcb6f4f85c1cf754b1d85a088b"><td class="mdescLeft">&#160;</td><td class="mdescRight"><code>init_all_mpi</code> This subroutine initializes <code>MPI</code> and finds out the number of processors <code>mpi_nprocs</code> as well as the index of the current one <code>mpi_myproc</code>. The flag <code>wflag</code> is set to true only for the processor numbered 0.  <a href="namespaceparallel.html#aa3a47cbcb6f4f85c1cf754b1d85a088b">More...</a><br /></td></tr>
<tr class="separator:aa3a47cbcb6f4f85c1cf754b1d85a088b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aac16d0a02c718c0fb956e02b986b638d"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#aac16d0a02c718c0fb956e02b986b638d">associate_nodes</a></td></tr>
<tr class="memdesc:aac16d0a02c718c0fb956e02b986b638d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The first loop in this subroutine distributes the wave functions over the nodes. This is done by looping over the wave functions and assigning one to each processor in turn. When the number of processors has been reached, it restarts from processor 0. This way of allocation is to some extent arbitrary and can be changed.  <a href="namespaceparallel.html#aac16d0a02c718c0fb956e02b986b638d">More...</a><br /></td></tr>
<tr class="separator:aac16d0a02c718c0fb956e02b986b638d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1608a6c6b47142227f92bf4cc49e8268"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a1608a6c6b47142227f92bf4cc49e8268">collect_densities</a></td></tr>
<tr class="memdesc:a1608a6c6b47142227f92bf4cc49e8268"><td class="mdescLeft">&#160;</td><td class="mdescRight">This subroutine uses the <code>MPI</code> routine <code>mpi_allreduce</code> to sum up the partial densities from the different nodes, using temporary arrays <code>tmp_rho</code> and <code>tmp_current</code> (depending on whether it is a scalar or vector field) in the process.  <a href="namespaceparallel.html#a1608a6c6b47142227f92bf4cc49e8268">More...</a><br /></td></tr>
<tr class="separator:a1608a6c6b47142227f92bf4cc49e8268"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0a509a54da5b66697fd0276110148543"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a0a509a54da5b66697fd0276110148543">collect_sp_properties</a></td></tr>
<tr class="memdesc:a0a509a54da5b66697fd0276110148543"><td class="mdescLeft">&#160;</td><td class="mdescRight">This subroutine collects the single-particle properties calculated from the wave functions and thus available only for the local wave functions on each node.  <a href="namespaceparallel.html#a0a509a54da5b66697fd0276110148543">More...</a><br /></td></tr>
<tr class="separator:a0a509a54da5b66697fd0276110148543"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a23978d1e63ec00b9bd3dd670eaf2a599"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a23978d1e63ec00b9bd3dd670eaf2a599">finish_mpi</a></td></tr>
<tr class="memdesc:a23978d1e63ec00b9bd3dd670eaf2a599"><td class="mdescLeft">&#160;</td><td class="mdescRight">This is just a wrapper for the <code>MPI</code> finalization call.  <a href="namespaceparallel.html#a23978d1e63ec00b9bd3dd670eaf2a599">More...</a><br /></td></tr>
<tr class="separator:a23978d1e63ec00b9bd3dd670eaf2a599"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5274b306c0464a4e2ff54c06fee5cf43"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a5274b306c0464a4e2ff54c06fee5cf43">mpi_init</a> (ierror)</td></tr>
<tr class="memdesc:a5274b306c0464a4e2ff54c06fee5cf43"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#a5274b306c0464a4e2ff54c06fee5cf43">More...</a><br /></td></tr>
<tr class="separator:a5274b306c0464a4e2ff54c06fee5cf43"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a72000f1fe30f921830c34726bfc48d57"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a72000f1fe30f921830c34726bfc48d57">mpi_comm_size</a> (comm_world, nprocs, ierror)</td></tr>
<tr class="memdesc:a72000f1fe30f921830c34726bfc48d57"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#a72000f1fe30f921830c34726bfc48d57">More...</a><br /></td></tr>
<tr class="separator:a72000f1fe30f921830c34726bfc48d57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae96c33bcbbf241f5e3ef41b525d44f36"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#ae96c33bcbbf241f5e3ef41b525d44f36">mpi_comm_rank</a> (comm_world, myproc, ierror)</td></tr>
<tr class="memdesc:ae96c33bcbbf241f5e3ef41b525d44f36"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#ae96c33bcbbf241f5e3ef41b525d44f36">More...</a><br /></td></tr>
<tr class="separator:ae96c33bcbbf241f5e3ef41b525d44f36"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a74c997f5a6f0b3d6cee5447c8e7a287a"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a74c997f5a6f0b3d6cee5447c8e7a287a">mpi_get_processor_name</a> (<a class="el" href="namespaceparallel.html#a37d1ce75101788b75c2ed15cae88dcf5">processor_name</a>, <a class="el" href="namespaceparallel.html#afe3fc05e32b34e42b81fab02f26bbd57">proc_namelen</a>, ierror)</td></tr>
<tr class="memdesc:a74c997f5a6f0b3d6cee5447c8e7a287a"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#a74c997f5a6f0b3d6cee5447c8e7a287a">More...</a><br /></td></tr>
<tr class="separator:a74c997f5a6f0b3d6cee5447c8e7a287a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2b5b4fe98b4e7c2af82404aa78215255"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a2b5b4fe98b4e7c2af82404aa78215255">mpi_barrier</a> (comm_world, ierror)</td></tr>
<tr class="memdesc:a2b5b4fe98b4e7c2af82404aa78215255"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#a2b5b4fe98b4e7c2af82404aa78215255">More...</a><br /></td></tr>
<tr class="separator:a2b5b4fe98b4e7c2af82404aa78215255"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a88b2a7834b1887a9e74cf410e6362af9"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a88b2a7834b1887a9e74cf410e6362af9">mpi_allreduce</a> (rho, tmp_rho, length, i_double_precision, sum, comm_world, ierror)</td></tr>
<tr class="memdesc:a88b2a7834b1887a9e74cf410e6362af9"><td class="mdescLeft">&#160;</td><td class="mdescRight">dummy function for the MPI routine  <a href="namespaceparallel.html#a88b2a7834b1887a9e74cf410e6362af9">More...</a><br /></td></tr>
<tr class="separator:a88b2a7834b1887a9e74cf410e6362af9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:aea8cd29f6c725f7bf461446bee12cbe4"><td class="memItemLeft" align="right" valign="top">logical, parameter&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#aea8cd29f6c725f7bf461446bee12cbe4">tmpi</a> =.TRUE.</td></tr>
<tr class="memdesc:aea8cd29f6c725f7bf461446bee12cbe4"><td class="mdescLeft">&#160;</td><td class="mdescRight">a logical variable set to true if <code>MPI</code> parallelization is activated. It is used to turn the calling of all the <code>MPI</code> routines in the code on or off.  <a href="namespaceparallel.html#aea8cd29f6c725f7bf461446bee12cbe4">More...</a><br /></td></tr>
<tr class="separator:aea8cd29f6c725f7bf461446bee12cbe4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac01189091dd5785f0aa2cd2eccf49da5"><td class="memItemLeft" align="right" valign="top">integer, dimension(:), allocatable&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#ac01189091dd5785f0aa2cd2eccf49da5">node</a></td></tr>
<tr class="memdesc:ac01189091dd5785f0aa2cd2eccf49da5"><td class="mdescLeft">&#160;</td><td class="mdescRight">For the single-particle state with index <code>i</code> the wave function is stored on computing node <code>node(i)</code>.  <a href="namespaceparallel.html#ac01189091dd5785f0aa2cd2eccf49da5">More...</a><br /></td></tr>
<tr class="separator:ac01189091dd5785f0aa2cd2eccf49da5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a61bcbe51b5127c46aa0860c387408293"><td class="memItemLeft" align="right" valign="top">integer, dimension(:), allocatable&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a61bcbe51b5127c46aa0860c387408293">localindex</a></td></tr>
<tr class="memdesc:a61bcbe51b5127c46aa0860c387408293"><td class="mdescLeft">&#160;</td><td class="mdescRight">For the single-particle state with index <code>i</code> the wave function is stored on computing node <code>node(i)</code> and its index on that node is <code>localindex(i)</code>.  <a href="namespaceparallel.html#a61bcbe51b5127c46aa0860c387408293">More...</a><br /></td></tr>
<tr class="separator:a61bcbe51b5127c46aa0860c387408293"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a01015d2108512a98792e7f4c8a968974"><td class="memItemLeft" align="right" valign="top">integer, dimension(:), allocatable&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a01015d2108512a98792e7f4c8a968974">globalindex</a></td></tr>
<tr class="memdesc:a01015d2108512a98792e7f4c8a968974"><td class="mdescLeft">&#160;</td><td class="mdescRight">tells the index of the single-particle state in the whole array of <code>nstmax</code> states (it could be dimensioned <code>nstloc</code> but is dimensioned as <code>nstmax</code> to make its allocation simpler). So for wave function index <code>i</code> <em> on the local node</em>, <code> i=1..nstloc</code>, the single-particle energy must be obtained using <code> sp_energy(globalindex(i))</code>. <br  />
  <a href="namespaceparallel.html#a01015d2108512a98792e7f4c8a968974">More...</a><br /></td></tr>
<tr class="separator:a01015d2108512a98792e7f4c8a968974"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ade61ceedeca3207434bc1cd4d9547063"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#ade61ceedeca3207434bc1cd4d9547063">mpi_nprocs</a></td></tr>
<tr class="memdesc:ade61ceedeca3207434bc1cd4d9547063"><td class="mdescLeft">&#160;</td><td class="mdescRight">number of MPI processes.  <a href="namespaceparallel.html#ade61ceedeca3207434bc1cd4d9547063">More...</a><br /></td></tr>
<tr class="separator:ade61ceedeca3207434bc1cd4d9547063"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a941ffc085ec1201ea3d7f16de8aeea08"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a941ffc085ec1201ea3d7f16de8aeea08">mpi_ierror</a></td></tr>
<tr class="memdesc:a941ffc085ec1201ea3d7f16de8aeea08"><td class="mdescLeft">&#160;</td><td class="mdescRight">varable for error output of MPI routines.  <a href="namespaceparallel.html#a941ffc085ec1201ea3d7f16de8aeea08">More...</a><br /></td></tr>
<tr class="separator:a941ffc085ec1201ea3d7f16de8aeea08"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae3a0e84f3cb12698da32ceb07d09647e"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#ae3a0e84f3cb12698da32ceb07d09647e">mpi_myproc</a></td></tr>
<tr class="memdesc:ae3a0e84f3cb12698da32ceb07d09647e"><td class="mdescLeft">&#160;</td><td class="mdescRight">the number of the local MPI process  <a href="namespaceparallel.html#ae3a0e84f3cb12698da32ceb07d09647e">More...</a><br /></td></tr>
<tr class="separator:ae3a0e84f3cb12698da32ceb07d09647e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a37d1ce75101788b75c2ed15cae88dcf5"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a37d1ce75101788b75c2ed15cae88dcf5">processor_name</a></td></tr>
<tr class="separator:a37d1ce75101788b75c2ed15cae88dcf5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afe3fc05e32b34e42b81fab02f26bbd57"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#afe3fc05e32b34e42b81fab02f26bbd57">proc_namelen</a></td></tr>
<tr class="separator:afe3fc05e32b34e42b81fab02f26bbd57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aefce06d5c67ff6bf57317a43977b50d8"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#aefce06d5c67ff6bf57317a43977b50d8">mpi_comm_world</a></td></tr>
<tr class="separator:aefce06d5c67ff6bf57317a43977b50d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a161a973634596e8f992dddec0ffb7dc9"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a161a973634596e8f992dddec0ffb7dc9">mpi_sum</a></td></tr>
<tr class="separator:a161a973634596e8f992dddec0ffb7dc9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a14bf7e02e0c33115d9058ffd60561f8e"><td class="memItemLeft" align="right" valign="top">integer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel.html#a14bf7e02e0c33115d9058ffd60561f8e">mpi_double_precision</a></td></tr>
<tr class="separator:a14bf7e02e0c33115d9058ffd60561f8e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This module organizes the execution on distributed-memory machines using <code>MPI</code>. Its interface is made such that on sequential machines <code><a class="el" href="parallel_8f90.html">parallel.f90</a></code> can simply be replaced by a special version <code><a class="el" href="sequential_8f90.html">sequential.f90</a></code> which contains a definition of this module generating trivial data. </p>
<p>contains the same routines as in file <code><a class="el" href="parallel_8f90.html">parallel.f90</a></code>, but with mostly empty functions to enable sequential running.</p>
<p><code>MPI</code> parallelization is based on distributing the wave functions onto the different nodes. Thus all operations acting directly on the wave functions can be done in parallel, not only the time evolution by the application of the single-particle Hamiltonian, but also the summing up of the densities and currents over those wave function stored on the node. This means that only the final summation of the densities and the calculations done with them have to be communicated across the nodes.</p>
<p>It is important that the single-particle properties also defined in <code>Levels</code>, e.g. <code>sp_energy</code> are not split up up for the nodes but the full set is present on each node. The values are communicated by summing from all nodes with zeroes in those index positions not present on a particular one. This method of handling them avoids having to communicate many small arrays.</p>
<p>Since an efficient way of dealing with Gram-Schmidt orthogonalization in this case was yet not found, at present the code can be run in <code>MPI</code> parallel mode only for the dynamic case. </p>
</div><h2 class="groupheader">Function/Subroutine Documentation</h2>
<a id="af4d1baf7c89b4eb0a8ea52b0d0dffa04"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4d1baf7c89b4eb0a8ea52b0d0dffa04">&#9670;&nbsp;</a></span>alloc_nodes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::alloc_nodes</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This subroutine merely allocates the internal arrays of module <code>Parallel</code>. </p>

</div>
</div>
<a id="aac16d0a02c718c0fb956e02b986b638d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aac16d0a02c718c0fb956e02b986b638d">&#9670;&nbsp;</a></span>associate_nodes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::associate_nodes</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The first loop in this subroutine distributes the wave functions over the nodes. This is done by looping over the wave functions and assigning one to each processor in turn. When the number of processors has been reached, it restarts from processor 0. This way of allocation is to some extent arbitrary and can be changed. </p>
<p>The second loop then calculates which wave functions are present on the local node and records their index <code>gobalindex</code> in the complete sequence. The third loop sets up the reverse pointers <code>localindex</code>, which has to be done in a loop over all processors to set up the information for the proper global indices. </p>

</div>
</div>
<a id="a1608a6c6b47142227f92bf4cc49e8268"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1608a6c6b47142227f92bf4cc49e8268">&#9670;&nbsp;</a></span>collect_densities()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::collect_densities</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This subroutine uses the <code>MPI</code> routine <code>mpi_allreduce</code> to sum up the partial densities from the different nodes, using temporary arrays <code>tmp_rho</code> and <code>tmp_current</code> (depending on whether it is a scalar or vector field) in the process. </p>

</div>
</div>
<a id="a0a509a54da5b66697fd0276110148543"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0a509a54da5b66697fd0276110148543">&#9670;&nbsp;</a></span>collect_sp_properties()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::collect_sp_properties</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This subroutine collects the single-particle properties calculated from the wave functions and thus available only for the local wave functions on each node. </p>
<p>It uses a simple trick: the arrays like <code>sp_energy</code> are defined for the full set of indices but set to zero before the calculation of these properties. On each node then the local values are calculated but inserted at the proper index for the full set of wave functions. In this subroutine the results from all the nodes are added up using <code>mpi_reduce</code>, so that effectively for each index one node contributes the correct value and the others zeroes. This process sounds inefficient but considering the small size of the arrays that does not matter. </p>

</div>
</div>
<a id="a23978d1e63ec00b9bd3dd670eaf2a599"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a23978d1e63ec00b9bd3dd670eaf2a599">&#9670;&nbsp;</a></span>finish_mpi()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::finish_mpi</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This is just a wrapper for the <code>MPI</code> finalization call. </p>

</div>
</div>
<a id="aa3a47cbcb6f4f85c1cf754b1d85a088b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa3a47cbcb6f4f85c1cf754b1d85a088b">&#9670;&nbsp;</a></span>init_all_mpi()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::init_all_mpi</td>
        </tr>
      </table>
</div><div class="memdoc">

<p><code>init_all_mpi</code> This subroutine initializes <code>MPI</code> and finds out the number of processors <code>mpi_nprocs</code> as well as the index of the current one <code>mpi_myproc</code>. The flag <code>wflag</code> is set to true only for the processor numbered 0. </p>

</div>
</div>
<a id="a88b2a7834b1887a9e74cf410e6362af9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a88b2a7834b1887a9e74cf410e6362af9">&#9670;&nbsp;</a></span>mpi_allreduce()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_allreduce </td>
          <td>(</td>
          <td class="paramtype">real(db), dimension(*), intent(in)&#160;</td>
          <td class="paramname"><em>rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(db), dimension(*), intent(in)&#160;</td>
          <td class="paramname"><em>tmp_rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>length</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>i_double_precision</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>sum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>comm_world</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<a id="a2b5b4fe98b4e7c2af82404aa78215255"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2b5b4fe98b4e7c2af82404aa78215255">&#9670;&nbsp;</a></span>mpi_barrier()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_barrier </td>
          <td>(</td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>comm_world</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<a id="ae96c33bcbbf241f5e3ef41b525d44f36"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae96c33bcbbf241f5e3ef41b525d44f36">&#9670;&nbsp;</a></span>mpi_comm_rank()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_comm_rank </td>
          <td>(</td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>comm_world</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>myproc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<a id="a72000f1fe30f921830c34726bfc48d57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a72000f1fe30f921830c34726bfc48d57">&#9670;&nbsp;</a></span>mpi_comm_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_comm_size </td>
          <td>(</td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>comm_world</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>nprocs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<a id="a74c997f5a6f0b3d6cee5447c8e7a287a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a74c997f5a6f0b3d6cee5447c8e7a287a">&#9670;&nbsp;</a></span>mpi_get_processor_name()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_get_processor_name </td>
          <td>(</td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>processor_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>proc_namelen</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<a id="a5274b306c0464a4e2ff54c06fee5cf43"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5274b306c0464a4e2ff54c06fee5cf43">&#9670;&nbsp;</a></span>mpi_init()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine parallel::mpi_init </td>
          <td>(</td>
          <td class="paramtype">integer&#160;</td>
          <td class="paramname"><em>ierror</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>dummy function for the MPI routine </p>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a01015d2108512a98792e7f4c8a968974"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a01015d2108512a98792e7f4c8a968974">&#9670;&nbsp;</a></span>globalindex</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer, dimension(:), allocatable parallel::globalindex</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>tells the index of the single-particle state in the whole array of <code>nstmax</code> states (it could be dimensioned <code>nstloc</code> but is dimensioned as <code>nstmax</code> to make its allocation simpler). So for wave function index <code>i</code> <em> on the local node</em>, <code> i=1..nstloc</code>, the single-particle energy must be obtained using <code> sp_energy(globalindex(i))</code>. <br  />
 </p>

</div>
</div>
<a id="a61bcbe51b5127c46aa0860c387408293"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a61bcbe51b5127c46aa0860c387408293">&#9670;&nbsp;</a></span>localindex</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer, dimension(:), allocatable parallel::localindex</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>For the single-particle state with index <code>i</code> the wave function is stored on computing node <code>node(i)</code> and its index on that node is <code>localindex(i)</code>. </p>

</div>
</div>
<a id="aefce06d5c67ff6bf57317a43977b50d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aefce06d5c67ff6bf57317a43977b50d8">&#9670;&nbsp;</a></span>mpi_comm_world</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_comm_world</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a14bf7e02e0c33115d9058ffd60561f8e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a14bf7e02e0c33115d9058ffd60561f8e">&#9670;&nbsp;</a></span>mpi_double_precision</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_double_precision</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a941ffc085ec1201ea3d7f16de8aeea08"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a941ffc085ec1201ea3d7f16de8aeea08">&#9670;&nbsp;</a></span>mpi_ierror</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_ierror</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>varable for error output of MPI routines. </p>

</div>
</div>
<a id="ae3a0e84f3cb12698da32ceb07d09647e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae3a0e84f3cb12698da32ceb07d09647e">&#9670;&nbsp;</a></span>mpi_myproc</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_myproc</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>the number of the local MPI process </p>

</div>
</div>
<a id="ade61ceedeca3207434bc1cd4d9547063"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ade61ceedeca3207434bc1cd4d9547063">&#9670;&nbsp;</a></span>mpi_nprocs</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_nprocs</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>number of MPI processes. </p>

</div>
</div>
<a id="a161a973634596e8f992dddec0ffb7dc9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a161a973634596e8f992dddec0ffb7dc9">&#9670;&nbsp;</a></span>mpi_sum</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::mpi_sum</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ac01189091dd5785f0aa2cd2eccf49da5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac01189091dd5785f0aa2cd2eccf49da5">&#9670;&nbsp;</a></span>node</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer, dimension(:), allocatable parallel::node</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>For the single-particle state with index <code>i</code> the wave function is stored on computing node <code>node(i)</code>. </p>

</div>
</div>
<a id="afe3fc05e32b34e42b81fab02f26bbd57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afe3fc05e32b34e42b81fab02f26bbd57">&#9670;&nbsp;</a></span>proc_namelen</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::proc_namelen</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a37d1ce75101788b75c2ed15cae88dcf5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a37d1ce75101788b75c2ed15cae88dcf5">&#9670;&nbsp;</a></span>processor_name</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">integer parallel::processor_name</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aea8cd29f6c725f7bf461446bee12cbe4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea8cd29f6c725f7bf461446bee12cbe4">&#9670;&nbsp;</a></span>tmpi</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">logical parameter parallel::tmpi =.TRUE.</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>a logical variable set to true if <code>MPI</code> parallelization is activated. It is used to turn the calling of all the <code>MPI</code> routines in the code on or off. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespaceparallel.html">parallel</a></li>
    <li class="footer">Generated by <a href="http://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.8.20 </li>
  </ul>
</div>
</body>
</html>
